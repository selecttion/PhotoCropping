import csv

from PIL import Image
import time
import os
from PyQt5.QtCore import QThread, pyqtSignal
from datetime import datetime
import sys
import cv2
import numpy as np
from skimage.metrics import structural_similarity as ssim
from skimage.feature import local_binary_pattern
from typing import List, Tuple


def get_project_root():
    """
    è¿”å›é¡¹ç›®æ ¹ç›®å½•
    - æºç è¿è¡Œ
    - PyInstaller exe
    éƒ½èƒ½ç”¨
    """
    if hasattr(sys, '_MEIPASS'):
        return sys._MEIPASS
    return os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))

# def load_model(weights_path):
#     # ç›´æ¥åŠ è½½æœ¬åœ°æ¨¡å‹æ–‡ä»¶
#     model = torch.hub.load('D:\work\PhotoCropping\yolov5', 'custom', path=weights_path, source='local')
#     # ç§»åŠ¨æ¨¡å‹åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆGPU æˆ– CPUï¼‰
#     model.to(device).eval()  # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
#     return model
def get_project_root():
    if hasattr(sys, '_MEIPASS'):
        return sys._MEIPASS
    return os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))

PROJECT_ROOT = get_project_root()
YOLOV5_ROOT = os.path.join(PROJECT_ROOT, "yolov5")

if YOLOV5_ROOT not in sys.path:
    sys.path.insert(0, YOLOV5_ROOT)

import torch
from yolov5.models.common import DetectMultiBackend

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def load_model(weight_name):
    weights_path = os.path.join(PROJECT_ROOT, "models", weight_name)

    model = DetectMultiBackend(
        weights_path,
        device=device,
        dnn=False,
        data=None,
        fp16=False
    )
    model.model.eval()
    return model

face_model = load_model("best.pt")
person_model = load_model("yolov5s.pt")

RATIO_MAP = {1: (7, 10, 1050, 1500), 2: (2, 3, 1200, 1800), 3: (3, 4, 1350, 1800)}
pid = 0


def load_image(image_path):
    img = cv2.imread(image_path)
    if img is None:
        print(f"âš ï¸ è­¦å‘Šï¼šæ— æ³•è¯»å– {image_path}ï¼Œå°è¯•ä½¿ç”¨ PIL è¯»å–ï¼")
        from PIL import Image
        img = Image.open(image_path).convert("RGB")  # ç¡®ä¿æ˜¯RGBæ ¼å¼
        img = np.array(img)  # è½¬æ¢ä¸º NumPy æ•°ç»„
    return img


class ImageData:
    """ å­˜å‚¨å•ä¸ªå›¾åƒä¿¡æ¯ """

    def __init__(self, image_id, image_path):
        self.image_id = image_id  # å›¾åƒé¡ºä½id
        self.image_path = image_path  # å›¾åƒåœ°å€
        try:
            with Image.open(image_path) as img:
                self.image = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)  # è¯»å–å›¾åƒå¹¶è½¬æ¢ä¸º OpenCV æ ¼å¼
                self.width, self.height = img.size  # ç›´æ¥è·å–å›¾åƒå°ºå¯¸
        except Exception as e:
            print(f"âš ï¸  è­¦å‘Š: è¯»å–å›¾åƒ {image_path} å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯: {e}")
            self.width, self.height = None, None
        self.max_crop_width = None  # æœ€å¤§åŒ–è£å‰ªé•¿å®½
        self.max_crop_height = None
        self.need_crop = 0  # 0: ä¸è£å‰ª, 1: è£å‰ª, 2: ç•™ç™½
        self.has_person = False  # æ˜¯å¦æœ‰äººç‰©
        self.person_count = 0  # äººç‰©æ•°é‡
        self.subject_coordinates = (0, 0)  # ä¸»ä½“ä¸­å¿ƒåæ ‡
        self.composition_choice = (0, 0)  # æ„å›¾é€‰æ‹©
        self.face_list = []  # å­˜å‚¨æ£€æµ‹åˆ°çš„ face è¾¹ç•Œæ¡† (xmin, ymin, xmax, ymax)
        self.person_list = []  # å­˜å‚¨æ£€æµ‹åˆ°çš„ person è¾¹ç•Œæ¡† (xmin, ymin, xmax, ymax)
        self.crop_coordinates = None  # è£å‰ªæ¡†åæ ‡
        self.loss_values = []  # å›¾åƒæŸå¤±é‡
        self.image_crop = None  # è£å‰ªåå›¾åƒæ•°æ®
        self.start_time = None  # å¼€å§‹æ—¶é—´
        self.end_time = None  # ç»“æŸæ—¶é—´


class CroppingThread(QThread):
    # ä¿¡å·
    log_signal = pyqtSignal(str)  # æ—¥å¿—æ¶ˆæ¯ä¿¡å·
    plabel_signal = pyqtSignal(str)  # æ›´æ–° label ä¿¡å·
    finished_signal = pyqtSignal()  # ä»»åŠ¡å®Œæˆä¿¡å·
    progress_signal = pyqtSignal(int)  # å®šä¹‰è¿›åº¦ä¿¡å·

    def __init__(self, user_setting):
        super().__init__()
        self.user_setting = user_setting  # ç°åœ¨ work.py å¯ä»¥è®¿é—® user_setting
        self.is_running = True
        self.image_list = []

    def run(self):
        """ çº¿ç¨‹æ‰§è¡Œçš„ä»»åŠ¡ """
        try:
            self.process_cropping()  # æ‰§è¡Œè£å‰ªä»»åŠ¡
        finally:
            if not self.is_running:
                self.log_signal.emit("â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\nå·²å–æ¶ˆ\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n\n")
                return
            if self.user_setting.number:
                self.progress_signal.emit(100)  # ç¡®ä¿ä»»åŠ¡å®Œæˆåè¿›åº¦æ¡æ»¡æ ¼
                # è®¡ç®—æ—¶é—´
                self.user_setting.end_time = datetime.now()
                all_time_ms = (self.user_setting.end_time - self.user_setting.start_time).total_seconds() * 1000
                average_time_ms = all_time_ms / self.user_setting.number

                self.log_signal.emit("â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”")
                self.log_signal.emit(f"ğŸ¯ ä»»åŠ¡å·²å®Œæˆï¼å¤„ç†{self.user_setting.number}å¼ å›¾ç‰‡ï¼Œ\nâ±å¹³å‡ç”¨æ—¶{average_time_ms:.3f} æ¯«ç§’ã€‚")
            self.finished_signal.emit()  # ä»»åŠ¡å®Œæˆåå‘é€ä¿¡å·

        # self.process_image_list(self.user_setting.output_path)

    def process_cropping(self):
        global pid
        self.plabel_signal.emit("è¿è¡Œing")
        print("å›¾åƒè£å‰ªå·¥ä½œå·²å¯åŠ¨...")
        self.progress_signal.emit(0)  # é‡ç½®è¿›åº¦æ¡
        self.user_setting.start_time = datetime.now()

        # è¾“å‡ºæ–‡ä»¶å¤¹æ£€æŸ¥
        if not os.path.exists(self.user_setting.input_path):
            try:
                os.makedirs(self.user_setting.output_path)
                self.log_signal.emit("è¾“å‡ºæ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œå·²è‡ªåŠ¨åˆ›å»ºã€‚")
            except Exception as e:
                self.log_signal.emit(f"é”™è¯¯: æ— æ³•åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹ï¼{str(e)}")
                self.log_signal.emit("å·²é€€å‡º")
                return

        # éå†è¾“å…¥æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰å›¾ç‰‡
        image_extensions = (".png", ".jpg", ".jpeg")
        image_id = 1

        self.log_signal.emit("å‘ç°å›¾åƒ: ")
        for root, _, files in os.walk(self.user_setting.input_path):
            for file in files:
                if not self.is_running:
                    return
                if file.lower().endswith(image_extensions):
                    image_path = os.path.join(root, file)

                    # åˆ›å»º ImageData å¯¹è±¡
                    image_data = ImageData(image_id, image_path)
                    self.image_list.append(image_data)

                    # å‘é€æ—¥å¿—ä¿¡å·
                    self.log_signal.emit(f"{image_path}")

                    image_id += 1  # é€’å¢ ID

        self.user_setting.number = len(self.image_list)

        if not self.user_setting.number:
            self.log_signal.emit("æœªæ‰¾åˆ°å›¾åƒ")
            return
        self.log_signal.emit(f"\nå‘ç° {self.user_setting.number} å¼ å›¾ç‰‡ï¼Œå‡†å¤‡è£å‰ª...\n")

        self.progress_signal.emit(5)  # å‘é€è¿›åº¦ä¿¡å·

        # ç•™ç™½æ¨¡å¼
        if self.user_setting.mode == 2:
            for image in self.image_list:
                image.start_time = datetime.now()
                self.add_white_padding(image)
                progress = int((image.image_id / self.user_setting.number * 95) + 5)  # è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”
                self.progress_signal.emit(progress)  # å‘é€è¿›åº¦ä¿¡å·
            self.progress_signal.emit(100)  # ç¡®ä¿ä»»åŠ¡å®Œæˆåè¿›åº¦æ¡æ»¡æ ¼
            return

        # è£å‰ªæ¨¡å¼
        batch_size = 4
        image_batches = [self.image_list[i:i + batch_size] for i in range(0, len(self.image_list), batch_size)]
        for batch in image_batches:
            if not self.is_running:
                break  # é€€å‡ºå¾ªç¯
            valid_images = []  # éœ€è¦è¿›è¡Œè£å‰ªçš„å›¾ç‰‡
            original_images = []  # å¯¹åº”çš„åŸå§‹å›¾åƒæ•°æ®

            # 1. **é¢„å¤„ç†é˜¶æ®µ**
            for image in batch:
                if not self.is_running:
                    break  # é€€å‡ºå¾ªç¯
                image.start_time = datetime.now()

                ratio_ok, resolution_ok = self.check_ratio_and_resolution(image)
                if ratio_ok and resolution_ok:
                    self.save_image(image=image)

                    pid = pid+1
                    progress = int((pid / self.user_setting.number * 95) + 5)  # è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”
                    self.progress_signal.emit(progress)  # å‘é€è¿›åº¦ä¿¡å·
                    # print(f"ç¬¬å‡ å¼ å›¾ï¼Ÿ{pid}")

                    image.need_crop = 0
                    continue

                elif not resolution_ok:
                    self.add_white_padding(image)  # è¿›è¡Œç•™ç™½

                    pid = pid + 1
                    progress = int((pid / self.user_setting.number * 95) + 5)  # è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”
                    self.progress_signal.emit(progress)  # å‘é€è¿›åº¦ä¿¡å·

                    image.need_crop = 2
                    continue

                self.compute_max_crop_size(image)

                valid_images.append(image)
                original_images.append(image)  # **å­˜å‚¨ image å¯¹è±¡å’Œå¯¹åº”çš„å›¾åƒæ•°æ®**
                # self.log_signal.emit(f"{image.image_path}ï¼šéœ€è¦å‰ª")
            if not valid_images:
                continue  # è¿™ä¸€æ‰¹å…¨æ˜¯ä¸éœ€è¦è£å‰ªçš„ï¼Œè·³è¿‡æ£€æµ‹æ­¥éª¤

            self.detect_faces_and_persons(original_images)

        print("å›¾åƒè£å‰ªå·¥ä½œå·²ç»“æŸ")
        pid = 0
        return

    def stop(self):
        self.is_running = False

    def save_image(self, image=None, new_img=None):
        """ ä¿å­˜å›¾åƒåˆ°ç›®æ ‡æ–‡ä»¶å¤¹ """
        try:
            if not os.path.exists(self.user_setting.output_path):
                os.makedirs(self.user_setting.output_path)  # å¦‚æœè¾“å‡ºæ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»º

            # è·å–å½“å‰æ—¶é—´æˆ³ï¼ˆç²¾ç¡®åˆ°ç§’ï¼‰
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            image_id = image.image_id  # ç›´æ¥ä» image è·å– ID

            if new_img is not None:
                pil_image = new_img
            elif image is not None:
                pil_image = Image.open(image.image_path)
            else:
                raise ValueError("ç¼ºå°‘å¿…è¦å‚æ•°ï¼šå¿…é¡»æä¾› image æˆ– (new_img å’Œ image)")

            # ç”ŸæˆåŸºç¡€æ–‡ä»¶å
            base_filename = f"cropped_{image_id}_{timestamp}.jpg"
            save_path = os.path.join(self.user_setting.output_path, base_filename)

            # å¤„ç†é‡åæƒ…å†µ
            counter = 1
            while os.path.exists(save_path):
                base_filename = f"cropped_{image_id}_{timestamp}({counter}).jpg"
                save_path = os.path.join(self.user_setting.output_path, base_filename)
                counter += 1

            image.end_time = datetime.now()
            elapsed_time_ms = (image.end_time - image.start_time).total_seconds() * 1000

            # ä¿å­˜å›¾åƒ
            pil_image.save(save_path, "JPEG", quality=95)  # ä»¥é«˜è´¨é‡ä¿å­˜

            self.log_signal.emit(f"âœ… å›¾åƒå·²ä¿å­˜: {save_path},\nâ±å¤„ç†ç”¨æ—¶{elapsed_time_ms:.3f} æ¯«ç§’")

        except Exception as e:
            print(f"âš ï¸ ä¿å­˜å›¾ç‰‡å¤±è´¥: {self.user_setting.output_path}, é”™è¯¯: {e}")

    def check_ratio_and_resolution(self, image_data):
        """æ£€æŸ¥å›¾åƒæ¯”ä¾‹ã€åˆ†è¾¨ç‡ï¼Œå¹¶è€ƒè™‘è£å‰ªæ–¹å‘"""
        ratio_key = self.user_setting.ratio
        if ratio_key not in RATIO_MAP:
            return False, False, False  # (æ¯”ä¾‹OK, åˆ†è¾¨ç‡OK, æ˜¯å¦éœ€è¦è£å‰ª)

        target_h, target_w, min_h, min_w = RATIO_MAP[ratio_key]
        img_w, img_h = image_data.width, image_data.height
        img_ratio = img_w / img_h  # å›¾ç‰‡çš„åŸå§‹æ¯”ä¾‹

        # å¤„ç†è£å‰ªæ¡†æ–¹å‘
        if self.user_setting.direction == 1:
            # åŸæ–¹å‘ï¼šè£å‰ªæ¡†è·Ÿéšå›¾ç‰‡æ–¹å‘
            if img_w >= img_h:
                target_ratio = target_w / target_h  # æ¨ªå›¾ï¼Œè£å‰ªæ¡†ä¹Ÿæ˜¯æ¨ªçš„
            else:
                target_ratio = target_h / target_w  # ç«–å›¾ï¼Œè£å‰ªæ¡†ä¹Ÿæ˜¯ç«–çš„
                min_w, min_h = min_h, min_w
        elif self.user_setting.direction == 2:
            # å¼ºåˆ¶æ¨ªå‘ï¼šä¸ç®¡å›¾ç‰‡å¦‚ä½•ï¼Œè£å‰ªæ¡†éƒ½å®½>é«˜
            target_ratio = max(target_w, target_h) / min(target_w, target_h)
        else:
            # å¼ºåˆ¶ç«–å‘ï¼šä¸ç®¡å›¾ç‰‡å¦‚ä½•ï¼Œè£å‰ªæ¡†éƒ½é«˜>å®½
            target_ratio = min(target_w, target_h) / max(target_w, target_h)
            min_w, min_h = min_h, min_w
        # è®¡ç®—æ˜¯å¦ç¬¦åˆæ¯”ä¾‹ & åˆ†è¾¨ç‡
        ratio_ok = abs(img_ratio - target_ratio) < 0.01
        resolution_ok = img_w >= (0.85 * min_w) and img_h >= (0.85 * min_h)

        return ratio_ok, resolution_ok

    def add_white_padding(self, image_data):
        """å¯¹åˆ†è¾¨ç‡ä¸å¤Ÿçš„å›¾åƒåŠ ç™½è¾¹"""
        img = Image.open(image_data.image_path)
        img_w, img_h = img.size

        # ç›®æ ‡æ¯”ä¾‹ & æœ€å°å®½é«˜
        ratio_key = self.user_setting.ratio
        target_h, target_w, min_h, min_w = RATIO_MAP.get(ratio_key, (7, 10, 1050, 1500))
        target_ratio = target_w / target_h
        original_ratio = img_w / img_h

        print(f"{image_data.image_id}çš„æ¯”ä¾‹{original_ratio}")

        # è®¡ç®—æ–°å°ºå¯¸
        if self.user_setting.direction == 1:  # åŸæ–¹å‘
            if original_ratio >= 1:  # å®½é•¿å‹
                if original_ratio > target_ratio and original_ratio != 1:  # ç«–çš„
                    new_w = int(img_w * 1.1)
                    new_h = int(new_w / target_ratio)
                    print(f"{image_data.image_id}çš„1.1,{new_w}å’Œ{new_h}")
                else:  # æ¨ªçš„
                    new_h = int(img_h * 1.1)
                    new_w = int(new_h * target_ratio)
                    print(f"{image_data.image_id}çš„1.2,{new_w}å’Œ{new_h}")
            else:  # é«˜é•¿å‹
                if original_ratio < (1 / target_ratio):
                    new_h = int(img_h * 1.1)
                    new_w = int(new_h * (1 / target_ratio))
                    print(f"{image_data.image_id}çš„2.1,{new_w}å’Œ{new_h}")
                else:
                    new_w = int(img_w * 1.1)
                    new_h = int(new_w / (1 / target_ratio))
                    print(f"{image_data.image_id}çš„2.2,{new_w}å’Œ{new_h}")

        elif self.user_setting.direction == 2:  # å¼ºåˆ¶æ¨ªå‘
            if original_ratio > target_ratio:
                new_w = int(img_w * 1.1)
                new_h = int(new_w / target_ratio)
            else:
                new_h = int(img_h * 1.1)
                new_w = int(new_h * target_ratio)
        else:  # å¼ºåˆ¶ç«–å‘
            if original_ratio < (1 / target_ratio):
                new_h = int(img_h * 1.1)
                new_w = int(new_h * (1 / target_ratio))
            else:
                new_w = int(img_w * 1.1)
                new_h = int(new_w / (1 / target_ratio))

        # åˆ›å»ºç™½è‰²èƒŒæ™¯çš„æ–°å›¾åƒ
        new_img = Image.new("RGB", (new_w, new_h), (255, 255, 255))

        # è®¡ç®—ç²˜è´´ä½ç½®ï¼Œä¿æŒå±…ä¸­
        paste_x = (new_w - img_w) // 2
        paste_y = (new_h - img_h) // 2
        new_img.paste(img, (paste_x, paste_y))

        self.save_image(image=image_data, new_img=new_img)

    def compute_max_crop_size(self, image_data):
        """è®¡ç®—ç¬¦åˆæ¯”ä¾‹çš„æœ€å¤§è£å‰ªæ¡†é•¿å®½"""
        img_w, img_h = image_data.width, image_data.height

        # è·å–ç›®æ ‡æ¯”ä¾‹ & æœ€å°å®½é«˜
        ratio_key = self.user_setting.ratio

        target_h, target_w, min_h, min_w = RATIO_MAP[ratio_key]
        target_ratio = target_w / target_h
        original_ratio = img_w / img_h

        if self.user_setting.direction == 1:  # åŸæ–¹å‘
            if original_ratio >= 1:  # å®½é•¿å‹
                if original_ratio > target_ratio:  # ç«–çš„
                    # å›¾ç‰‡è¾ƒå®½ï¼Œé™åˆ¶å®½åº¦
                    image_data.max_crop_height = img_h
                    image_data.max_crop_width = int(img_h * target_ratio)
                else:
                    # å›¾ç‰‡è¾ƒé«˜ï¼Œé™åˆ¶é«˜åº¦
                    image_data.max_crop_width = img_w
                    image_data.max_crop_height = int(img_w / target_ratio)
            else:
                if original_ratio < (1 / target_ratio):
                    image_data.max_crop_width = img_w
                    image_data.max_crop_height = int(img_w / (1 / target_ratio))
                else:
                    image_data.max_crop_height = img_h
                    image_data.max_crop_width = int(img_h * (1 / target_ratio))

        elif self.user_setting.direction == 2:  # å¼ºåˆ¶æ¨ªå‘
            if original_ratio > target_ratio:
                # å›¾ç‰‡è¾ƒå®½ï¼Œé™åˆ¶å®½åº¦
                image_data.max_crop_height = img_h
                image_data.max_crop_width = int(img_h * target_ratio)
            else:
                # å›¾ç‰‡è¾ƒé«˜ï¼Œé™åˆ¶é«˜åº¦
                image_data.max_crop_width = img_w
                image_data.max_crop_height = int(img_w / target_ratio)

        else:  # å¼ºåˆ¶ç«–å‘
            if original_ratio < (1 / target_ratio):
                image_data.max_crop_width = img_w
                image_data.max_crop_height = int(img_w / (1 / target_ratio))
            else:
                image_data.max_crop_height = img_h
                image_data.max_crop_width = int(img_h * (1 / target_ratio))

        print(f"{image_data.image_id}æœ€å¤§è£å‰ªå°ºå¯¸ï¼š{image_data.max_crop_width}x{image_data.max_crop_height}")

    def calculate_composition_choice(self, image_data):
        """ è®¡ç®—ç›®æ ‡ç‚¹åœ¨æ„å›¾ä¸‰åˆ†çº¿ä¸Šæœ€æ¥è¿‘çš„ä½ç½® """

        img_w, img_h = image_data.width, image_data.height
        target_x, target_y = image_data.subject_coordinates
        # è®¡ç®— x æ–¹å‘ä¸Šçš„ä¸‰åˆ†çº¿
        thirds_x = [img_w / 3, img_w / 2, 2 * img_w / 3]  # [1/3çº¿, ä¸­ç‚¹, 2/3çº¿]

        # è®¡ç®— y æ–¹å‘ä¸Šçš„ä¸‰åˆ†çº¿
        thirds_y = [img_h / 3, img_h / 2, 2 * img_h / 3]  # [1/3çº¿, ä¸­ç‚¹, 2/3çº¿]

        # è®¡ç®— target_x åˆ°ä¸‰åˆ†çº¿çš„è·ç¦»ï¼Œé€‰å–æœ€è¿‘çš„
        distances_x = [abs(target_x - pos) for pos in thirds_x]
        closest_x = distances_x.index(min(distances_x)) + 1  # +1 æ˜¯å› ä¸ºç´¢å¼• 0 å¯¹åº” 1/3 çº¿ï¼Œç´¢å¼• 1 å¯¹åº” 1/2 çº¿

        # è®¡ç®— target_y åˆ°ä¸‰åˆ†çº¿çš„è·ç¦»ï¼Œé€‰å–æœ€è¿‘çš„
        distances_y = [abs(target_y - pos) for pos in thirds_y]
        closest_y = distances_y.index(min(distances_y)) + 1  # +1 æ˜¯å› ä¸ºç´¢å¼• 0 å¯¹åº” 1/3 çº¿ï¼Œç´¢å¼• 1 å¯¹åº” 1/2 çº¿

        image_data.composition_choice = (closest_x, closest_y)
        return closest_x, closest_y

    def crop_image_with_composition(self, image_data):
        """
        æ ¹æ®æ„å›¾é€‰æ‹©è£å‰ªå›¾ç‰‡ï¼Œå¹¶ä½¿è£å‰ªæ¡†å°½é‡ä¸ç›®æ ‡ç‚¹å¯¹é½ã€‚

        å‚æ•°ï¼š
            image_data: å›¾ç‰‡å¯¹è±¡ï¼ŒåŒ…å«è·¯å¾„ã€IDã€å°ºå¯¸ã€æ„å›¾é€‰æ‹©ã€æœ€å¤§è£å‰ªå°ºå¯¸ç­‰ä¿¡æ¯ã€‚
            target_point: (x, y) åæ ‡ï¼Œè£å‰ªæ¡†åº”å°½é‡ä¸è¯¥ç‚¹å¯¹é½ã€‚
        """
        # åŠ è½½å›¾åƒ
        img = Image.open(image_data.image_path)
        img_w, img_h = img.size

        # è·å–è£å‰ªæ¡†å°ºå¯¸
        crop_w, crop_h = image_data.max_crop_width, image_data.max_crop_height
        target_x, target_y = image_data.subject_coordinates  # ç›®æ ‡ç‚¹åæ ‡

        # è·å–æ„å›¾åç§»é‡
        composition_x_map = {0: 0, 1: 1 / 3, 2: 1 / 2, 3: 2 / 3}
        composition_y_map = {0: 0, 1: 1 / 3, 2: 1 / 2, 3: 2 / 3}

        offset_x = composition_x_map.get(image_data.composition_choice[0], 0)
        offset_y = composition_y_map.get(image_data.composition_choice[1], 0)

        # è®¡ç®—è£å‰ªæ¡†å·¦ä¸Šè§’åæ ‡
        crop_x = int(target_x - offset_x * crop_w)
        crop_y = int(target_y - offset_y * crop_h)

        # **è¾¹ç•Œæ£€æŸ¥ï¼Œç¡®ä¿è£å‰ªæ¡†ä¸ä¼šè¶…å‡ºåŸå›¾**
        crop_x = max(0, min(crop_x, img_w - crop_w))
        crop_y = max(0, min(crop_y, img_h - crop_h))

        image_data.crop_coordinates = (crop_x, crop_y, crop_x + crop_w, crop_y + crop_h)
        print("è£å‰ªï¼š", image_data.crop_coordinates)

        # è£å‰ªå›¾ç‰‡
        cropped_img = img.crop((crop_x, crop_y, crop_x + crop_w, crop_y + crop_h))
        image_data.image_crop = cropped_img
        self.save_image(image=image_data, new_img=cropped_img)

        # æŸå¤±é‡è¯„ä¼°
        # self.compute_ssim_rgb(image_data)

    def detect_faces_and_persons(self, image_data_pairs):
        """ æ‰¹é‡æ£€æµ‹å›¾ç‰‡ä¸­çš„äººè„¸å’Œäººä½“ï¼Œå¹¶å­˜å‚¨ç»“æœåˆ° image ç»“æ„ä½“ """
        global pid
        img_list = []
        image_dict = {}  # å­˜å‚¨ image_id -> image ç»“æ„ä½“çš„æ˜ å°„

        # **1. æ‰¹é‡é¢„å¤„ç†**
        for image in image_data_pairs:
            image_dict[image.image_id] = image  # æŒ‰ image_id å­˜å…¥å­—å…¸
            # print(f"åŠ {image.image_path}è¿›å»")

            img_list.append(image.image_path)

        if not img_list:
            return []  # é¿å…ç©ºæ‰¹é‡æŠ¥é”™

        print(f"è¾“å…¥çš„å›¾åƒæ•°é‡: {len(img_list)}")
        # **2. è¿›è¡Œæ‰¹é‡æ£€æµ‹**
        face_results = face_model(img_list)
        person_results = person_model(img_list)

        # print(f"face_results åˆ—è¡¨çš„é•¿åº¦: {len(face_results)}")
        # print(f"face_results åˆ—è¡¨: {face_results}")

        for i in range(len(image_dict)):  # éå†æ¯ä¸€å¼ å›¾ç‰‡çš„æ£€æµ‹ç»“æœ
            if not self.is_running:
                break  # é€€å‡ºå¾ªç¯
            image_id = list(image_dict.keys())[i]  # è·å– image_id
            image = image_dict[image_id]  # é€šè¿‡ image_id è·å–å¯¹åº”çš„ image ç»“æ„ä½“
            print(f"å¤„ç†å›¾ç‰‡ï¼š{image_id}")

            # å‘é€è¿›åº¦ä¿¡å·
            progress = int(pid / self.user_setting.number * 95 + (95 / self.user_setting.number * 0.3) + 5)
            self.progress_signal.emit(progress)

            # è·å–å½“å‰å›¾ç‰‡å¯¹åº”çš„äººè„¸æ£€æµ‹ç»“æœ
            face_det = face_results.xyxy[i].clone()  # å…‹éš†å¼ é‡ï¼Œé¿å… in-place ä¿®æ”¹é”™è¯¯
            person_det = person_results.xyxy[i].clone()  # è·å–å½“å‰å›¾ç‰‡çš„äººä½“æ£€æµ‹ç»“æœ
            # print(face_det)
            # print(person_det)

            # å…ˆå°† face_det çš„ class_id è½¬æ¢ä¸º int
            face_det[:, 5] = face_det[:, 5].int()

            # è§£æ face_detï¼šç­›é€‰ class_id=0 ä¸”ç½®ä¿¡åº¦ > 0.4
            face_mask = (face_det[:, 5] == 0) & (face_det[:, 4] > 0.4)
            face_filtered = face_det[face_mask]  # å–ç¬¦åˆæ¡ä»¶çš„æ¡†

            if len(face_filtered) > 0:
                image.face_list = face_filtered[:, :4].int().tolist()  # è½¬æ¢ä¸º int å¹¶å­˜å…¥ [x1, y1, x2, y2, class]
            else:
                image.face_list = []

            # è§£æ person_detï¼šå…ˆå°† class_id è½¬æ¢ä¸º int
            person_det[:, 5] = person_det[:, 5].int()
            person_mask = person_det[:, 5] == 0
            persons_filtered = person_det[person_mask]

            if len(persons_filtered) > 0:
                # è®¡ç®—æ¯ä¸ªæ¡†çš„é¢ç§¯ (width * height)
                areas = (persons_filtered[:, 2] - persons_filtered[:, 0]) * (
                        persons_filtered[:, 3] - persons_filtered[:, 1])

                # è®¡ç®—åŸå›¾åƒé¢ç§¯
                img_area = image.width * image.height

                # ç­›é€‰å‡ºé¢ç§¯å¤§äºåŸå›¾åƒ 1% çš„æ£€æµ‹æ¡†
                valid_mask = areas > (0.01 * img_area)
                persons_valid = persons_filtered[valid_mask]

                if len(persons_valid) > 0:
                    image.person_list = persons_valid[:, :4].int().tolist()  # å– x1, y1, x2, y2 å¹¶è½¬æ¢ä¸º int
                    image.has_person = True
                else:
                    image.person_list = []
            else:
                image.person_list = []  # æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æ¡†ï¼Œç½®ç©º

            # print(f"æœ‰äººå—ï¼Ÿ{image.has_person}")
            # print(f"äººï¼š{image.person_list}")
            #
            # # è°ƒç”¨ç»˜åˆ¶æ¡†çš„å‡½æ•°
            # image_with_boxes = self.draw_boxes(image)
            #
            # if image_with_boxes is not None:
            #     # è°ƒæ•´å›¾åƒå°ºå¯¸ï¼Œä½¿å…¶é€‚åˆå±å¹•
            #     image_resized = self.resize_image(image_with_boxes)
            #
            #     # æ˜¾ç¤ºç¼©æ”¾åçš„å›¾åƒ
            #     cv2.imshow('Detection Preview', image_resized)
            #     cv2.waitKey(0)  # ç­‰å¾…æŒ‰é”®è¾“å…¥
            #     cv2.destroyAllWindows()  # å…³é—­æ‰€æœ‰ OpenCV çª—å£

            self.select_subject_and_crop(image)

        return list(image_dict.values())  # è¿”å›æ›´æ–°åçš„ image ç»“æ„ä½“åˆ—è¡¨

    def resize_image(self, image, max_width=1600, max_height=1000):
        """
        å°†å›¾åƒç¼©æ”¾åˆ°é€‚åº”å±å¹•çš„å¤§å°
        :param image: è¾“å…¥çš„å›¾åƒ
        :param max_width: å›¾åƒçš„æœ€å¤§å®½åº¦
        :param max_height: å›¾åƒçš„æœ€å¤§é«˜åº¦
        :return: ç¼©æ”¾åçš„å›¾åƒ
        """
        height, width = image.shape[:2]

        # å¦‚æœå›¾åƒå°ºå¯¸å·²ç»å°äºæœ€å¤§å°ºå¯¸ï¼Œåˆ™ä¸è¿›è¡Œç¼©æ”¾
        if width <= max_width and height <= max_height:
            return image

        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
        scale_width = max_width / width
        scale_height = max_height / height
        scale = min(scale_width, scale_height)

        # ç¼©æ”¾å›¾åƒ
        new_width = int(width * scale)
        new_height = int(height * scale)
        resized_image = cv2.resize(image, (new_width, new_height))

        return resized_image

    def draw_boxes(self, image_data):
        """
        ç»˜åˆ¶äººè„¸å’Œäººç‰©æ¡†
        :param image_data: åŒ…å«å›¾åƒå’Œæ£€æµ‹æ¡†çš„ ImageData å¯¹è±¡
        :return: ç»˜åˆ¶äº†æ¡†çš„å›¾åƒ
        """
        if image_data.image is None:
            print(f"âš ï¸ å›¾åƒ {image_data.image_path} åŠ è½½å¤±è´¥ï¼Œæ— æ³•ç»˜åˆ¶æ¡†ï¼")
            return None

        image = image_data.image  # è·å–å›¾åƒ

        # ç»˜åˆ¶äººè„¸æ¡†
        for box in image_data.face_list:
            x1, y1, x2, y2 = box
            cv2.rectangle(image, (x1, y1), (x2, y2), (255, 140, 0), 5)  # è“è‰²æ¡†

        # ç»˜åˆ¶äººä½“æ¡†
        for box in image_data.person_list:
            x1, y1, x2, y2 = box
            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 0, 255), 7)  # çº¢è‰²æ¡†

        return image

    def select_subject_and_crop(self, image_data):
        """
        æ ¹æ® face_list å’Œ person_list é€‰æ‹©ä¸»ä½“ç‰©ï¼Œå¹¶æ‰§è¡Œè£å‰ªæˆ–ç•™ç™½æ“ä½œ
        """
        global pid

        # è·å–å›¾åƒä¸­å¿ƒç‚¹åæ ‡
        center_x, center_y = image_data.width / 2, image_data.height / 2
        image_data.composition_choice = (2, 2)  # æš‚æ—¶å›ºå®šä¸º (2,2)

        # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°äººä½“ï¼Œåˆ™ç›´æ¥è¿”å›
        if not image_data.has_person:
            image_data.subject_coordinates = (center_x, center_y)
            self.crop_image_with_composition(image_data)
            return

        def calculate_weighted_score(box, face_boxes=None):
            """
            è®¡ç®—ç›®æ ‡æ¡†çš„åŠ æƒè¯„åˆ†ï¼š
            - å¯¹äº person æ¡†ï¼ŒåŸºäº **æœ€é è¿‘ä¸­å¿ƒç‚¹**ï¼ˆ50%ï¼‰+ **é¢ç§¯æœ€å¤§**ï¼ˆ50%ï¼‰ã€‚
            - å¯¹äºåŒ…å« face çš„ person æ¡†ï¼ŒåŸºäº **æœ€é è¿‘ä¸­å¿ƒç‚¹**ï¼ˆ50%ï¼‰+ **face é¢ç§¯æœ€å¤§**ï¼ˆ50%ï¼‰ã€‚
            """

            # è§£ææ¡†åæ ‡
            x_min, y_min, x_max, y_max = box
            box_center_x, box_center_y = (x_min + x_max) / 2, (y_min + y_max) / 2  # è®¡ç®—æ¡†çš„ä¸­å¿ƒç‚¹
            box_area = (x_max - x_min) * (y_max - y_min)  # è®¡ç®—æ¡†çš„é¢ç§¯

            # è®¡ç®—æ¡†ä¸å›¾åƒä¸­å¿ƒç‚¹çš„æ¬§å‡ é‡Œå¾—è·ç¦»
            distance_score = 1 / (
                        np.linalg.norm(np.array([box_center_x, box_center_y]) - np.array([center_x, center_y])) + 1e-6)

            # è®¡ç®—é¢ç§¯å æ¯”å¾—åˆ†
            area_score = box_area / (image_data.width * image_data.height)

            # å¦‚æœæä¾›äº† face_boxesï¼Œåˆ™è®¡ç®— face ç›¸å…³çš„å¾—åˆ†
            if face_boxes:
                face_overlap_scores = []  # å­˜å‚¨æ¯ä¸ª face çš„é‡å å¾—åˆ†
                for fx_min, fy_min, fx_max, fy_max in face_boxes:
                    face_area = (fx_max - fx_min) * (fy_max - fy_min)  # è®¡ç®— face æ¡†çš„é¢ç§¯
                    overlap_x_min, overlap_y_min = max(x_min, fx_min), max(y_min, fy_min)  # è®¡ç®—äº¤é›†åŒºåŸŸ
                    overlap_x_max, overlap_y_max = min(x_max, fx_max), min(y_max, fy_max)
                    overlap_area = max(0, overlap_x_max - overlap_x_min) * max(0, overlap_y_max - overlap_y_min)

                    # è®¡ç®— face è¢«åŒ…å«çš„æ¯”ä¾‹
                    overlap_ratio = overlap_area / face_area
                    face_overlap_scores.append(overlap_ratio)

                face_score = max(face_overlap_scores) if face_overlap_scores else 0  # é€‰å–æœ€å¤§ face è¦†ç›–ç‡
                return 0.4 * distance_score + 0.6 * area_score + 0.3 * face_score  # è®¡ç®—æœ€ç»ˆå¾—åˆ†
            else:
                return 0.5 * distance_score + 0.7 * area_score  # ä»…åŸºäºä¸­å¿ƒç‚¹å’Œé¢ç§¯è®¡ç®—å¾—åˆ†

        def is_face_inside_person(face_box, person_box, overlap_threshold=0.4):
            """
            åˆ¤æ–­ face_box æ˜¯å¦è¢« person_box åŒ…å«æˆ–å¤§éƒ¨åˆ†é‡åˆ
            - overlap_threshold: face æ¡†ä¸ person æ¡†çš„é‡å æ¯”ä¾‹ï¼ˆé»˜è®¤ä¸º 40%ï¼‰
            """
            fx_min, fy_min, fx_max, fy_max = face_box
            px_min, py_min, px_max, py_max = person_box

            # è®¡ç®— face æ¡†çš„é¢ç§¯
            face_area = (fx_max - fx_min) * (fy_max - fy_min)

            # è®¡ç®—äº¤é›†åŒºåŸŸ
            overlap_x_min = max(fx_min, px_min)
            overlap_y_min = max(fy_min, py_min)
            overlap_x_max = min(fx_max, px_max)
            overlap_y_max = min(fy_max, py_max)

            overlap_width = max(0, overlap_x_max - overlap_x_min)
            overlap_height = max(0, overlap_y_max - overlap_y_min)
            overlap_area = overlap_width * overlap_height

            # è®¡ç®— face æ¡†ä¸ person æ¡†çš„é‡å æ¯”ä¾‹
            overlap_ratio = overlap_area / face_area

            return overlap_ratio >= overlap_threshold

        # **æƒ…å†µ 1**ï¼šæ²¡æœ‰æ£€æµ‹åˆ°äººè„¸ï¼Œåªæœ‰ person æ¡†
        if not image_data.face_list:
            # è®¡ç®—æ‰€æœ‰ person æ¡†çš„å¾—åˆ†
            scores = [calculate_weighted_score(person_box) for person_box in image_data.person_list]
            best_idx = np.argmax(scores)  # é€‰å–å¾—åˆ†æœ€é«˜çš„æ¡†
            image_data.person_list = [image_data.person_list[best_idx]]  # ä»…ä¿ç•™ä¸»ä½“ç‰© person æ¡†

            # è®¡ç®—ä¸»ä½“ç‰©çš„ä¸­å¿ƒç‚¹åæ ‡
            x_min, y_min, x_max, y_max = image_data.person_list[0]
            image_data.subject_coordinates = ((x_min + x_max) / 2, (y_min + y_max) / 2)

        # **æƒ…å†µ 2**ï¼šæ£€æµ‹åˆ°äººè„¸ï¼Œé€‰æ‹©æœ€åŒ¹é…çš„ person æ¡†
        else:
            # è®¡ç®—æ‰€æœ‰ person æ¡†çš„å¾—åˆ†ï¼ˆè€ƒè™‘ face çš„å› ç´ ï¼‰
            scores = [calculate_weighted_score(person_box, image_data.face_list) for person_box in
                      image_data.person_list]
            print(f"{image_data.image_id}çš„åˆ†{scores}")
            best_idx = np.argmax(scores)  # é€‰å–å¾—åˆ†æœ€é«˜çš„æ¡†

            # æ‰¾å‡ºå¾—åˆ†ç›¸è¿‘çš„ person æ¡†ï¼ˆç›¸å·®å°äºï¼‰
            best_score = scores[best_idx]
            similar_boxes = [box for i, box in enumerate(image_data.person_list) if abs(scores[i] - best_score) < 0.045]

            # ä»…ä¿ç•™ç›¸è¿‘å¾—åˆ†çš„ä¸»ä½“ç‰©æ¡†
            image_data.person_list = similar_boxes

            # ç­›é€‰ face_list
            filtered_faces = []
            for face_box in image_data.face_list:
                # åªä¿ç•™è‡³å°‘æœ‰ä¸€ä¸ª person æ¡†åŒ…å«æˆ–å¤§éƒ¨åˆ†é‡åˆçš„ face
                if any(is_face_inside_person(face_box, person_box) for person_box in image_data.person_list):
                    filtered_faces.append(face_box)

            image_data.face_list = filtered_faces  # æ›´æ–° face_list

            # è®¡ç®—ä¸»ä½“ç‰©çš„ä¸­å¿ƒç‚¹åæ ‡
            if len(image_data.face_list) == 0 :
                x_min, y_min, x_max, y_max = image_data.person_list[0]
                image_data.subject_coordinates = ((x_min + x_max) / 2, (y_min + y_max) / 2)
            elif len(image_data.face_list) == 1 :
                # ä»…æœ‰ä¸€ä¸ª faceï¼Œè®¡ç®—å…¶ä¸»ä½“åæ ‡
                fx_min, fy_min, fx_max, fy_max = image_data.face_list[0]
                face_x = (fx_min + fx_max) / 2
                face_y = fy_min + (fy_max - fy_min) / 2  # å– face æ¡†çš„ 1/2 å¤„
                image_data.subject_coordinates = (face_x, face_y)
            else:
                # å¤šä¸ª faceï¼Œè®¡ç®—æ‰€æœ‰ face åæ ‡çš„å¹³å‡å€¼
                face_coords = []
                for fx_min, fy_min, fx_max, fy_max in image_data.face_list:
                    face_x = (fx_min + fx_max) / 2
                    face_y = fy_min + (fy_max - fy_min) / 2
                    face_coords.append((face_x, face_y))
                image_data.subject_coordinates = tuple(np.mean(face_coords, axis=0))

        # **æ‰§è¡Œè£å‰ª**

        # # è°ƒç”¨ç»˜åˆ¶æ¡†çš„å‡½æ•°
        # image_with_boxes = self.draw_boxes(image_data)
        # if image_with_boxes is not None:
        #     # è°ƒæ•´å›¾åƒå°ºå¯¸ï¼Œä½¿å…¶é€‚åˆå±å¹•
        #     image_resized = self.resize_image(image_with_boxes)
        #
        #     # æ˜¾ç¤ºç¼©æ”¾åçš„å›¾åƒ
        #     cv2.imshow('Detection Preview', image_resized)
        #     cv2.waitKey(0)  # ç­‰å¾…æŒ‰é”®è¾“å…¥
        #     cv2.destroyAllWindows()  # å…³é—­æ‰€æœ‰ OpenCV çª—å£

        # è®¡ç®—æ„å›¾é€‰æ‹©
        self.calculate_composition_choice(image_data)
        # å‘é€è¿›åº¦ä¿¡å·
        progress = int(pid / self.user_setting.number * 95 + (95 / self.user_setting.number * 0.6) + 5)
        self.progress_signal.emit(progress)

        if image_data.face_list:
            face_x_min = min(face[0] for face in image_data.face_list)  # æœ€å·¦ä¾§è¾¹ç•Œ
            face_y_min = min(face[1] for face in image_data.face_list)  # æœ€ä¸Šä¾§è¾¹ç•Œ
            face_x_max = max(face[2] for face in image_data.face_list)  # æœ€å³ä¾§è¾¹ç•Œ
            face_y_max = max(face[3] for face in image_data.face_list)  # æœ€ä¸‹ä¾§è¾¹ç•Œ

            face_width = face_x_max - face_x_min  # face æ•´ä½“åŒºåŸŸå®½åº¦
            face_height = face_y_max - face_y_min  # face æ•´ä½“åŒºåŸŸé«˜åº¦

            # **æ£€æŸ¥æ˜¯å¦å¯ä»¥è£å‰ª**
            if face_width <= image_data.max_crop_width and face_height <= image_data.max_crop_height:
                # **å¯ä»¥è£å‰ª**
                image_data.need_crop = 1
                self.crop_image_with_composition(image_data)
            else:
                # **face æ¡†å¤ªå¤§ï¼Œé¿å…è£å‰ªå½±å“ faceï¼Œæ”¹ä¸ºç•™ç™½**
                image_data.need_crop = 2
                self.add_white_padding(image_data)
        else:
            # **æ²¡æœ‰ face æ¡†ï¼Œç›´æ¥è£å‰ª**
            image_data.need_crop = 1
            self.crop_image_with_composition(image_data)

        # å‘é€è¿›åº¦ä¿¡å·
        pid = pid+1
        progress = int((pid / self.user_setting.number * 95) + 5)
        self.progress_signal.emit(progress)
        # print(f"ç¬¬å‡ å¼ å›¾ï¼Ÿ{pid}")

    def compute_ssim_rgb(self, image_data):
        """
            SSIMæŸå¤±é‡
        """
        image_data.image_crop = cv2.cvtColor(np.array(image_data.image_crop), cv2.COLOR_RGB2BGR)
        # ç¡®ä¿å›¾åƒå¤§å°ä¸€è‡´
        if image_data.image.shape != image_data.image_crop.shape:
            resized_crop = cv2.resize(image_data.image_crop, (image_data.image.shape[1], image_data.image.shape[0]))
        else:
            resized_crop = image_data.image_crop

        # åˆå§‹åŒ–å˜é‡
        scores = []

        # åˆ†åˆ«è®¡ç®—æ¯ä¸ªé€šé“çš„SSIM
        for i in range(3):  # BGR é¡ºåº
            score, _ = ssim(image_data.image[:, :, i], resized_crop[:, :, i], full=True)
            scores.append(score)

        avg_score = sum(scores) / 3
        print(f"RGBé€šé“å¹³å‡ SSIM: {avg_score:.4f}")

    # æŸå¤±é‡
    def compute_crop_loss(self, image: np.ndarray,
                          crop_coords: Tuple[int, int, int, int],
                          person_list: List[Tuple[int, int, int, int]]) -> dict:
        """
        è®¡ç®—è£å‰ªæŸå¤±ï¼šé¢œè‰²ã€çº¹ç†ã€äººåƒæŸå¤±
        """
        xmin, ymin, xmax, ymax = crop_coords
        crop = image[ymin:ymax, xmin:xmax]

        # é¢œè‰²æŸå¤±
        def color_hist_loss(img1, img2):
            hist1 = cv2.calcHist([img1], [0, 1, 2], None, [8, 8, 8], [0, 256] * 3)
            hist2 = cv2.calcHist([img2], [0, 1, 2], None, [8, 8, 8], [0, 256] * 3)
            hist1 = cv2.normalize(hist1, hist1).flatten()
            hist2 = cv2.normalize(hist2, hist2).flatten()
            return 1 - cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)

        color_loss = color_hist_loss(image, crop)

        # çº¹ç†æŸå¤±
        def texture_hist_loss(img1, img2):
            gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
            gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
            lbp1 = local_binary_pattern(gray1, P=8, R=1.0)
            lbp2 = local_binary_pattern(gray2, P=8, R=1.0)
            hist1, _ = np.histogram(lbp1.ravel(), bins=256, range=(0, 256), density=True)
            hist2, _ = np.histogram(lbp2.ravel(), bins=256, range=(0, 256), density=True)
            return np.sum(np.abs(hist1 - hist2)) / 2

        texture_loss = texture_hist_loss(image, crop)

        # personæŸå¤±
        def compute_person_loss(person_list, crop_box):
            crop_xmin, crop_ymin, crop_xmax, crop_ymax = crop_box
            crop_area = (crop_xmax - crop_xmin) * (crop_ymax - crop_ymin)
            total_person_area = 0
            cropped_person_area = 0
            for (xmin, ymin, xmax, ymax) in person_list:
                area = (xmax - xmin) * (ymax - ymin)
                total_person_area += area
                inter_xmin = max(xmin, crop_xmin)
                inter_ymin = max(ymin, crop_ymin)
                inter_xmax = min(xmax, crop_xmax)
                inter_ymax = min(ymax, crop_ymax)
                if inter_xmin < inter_xmax and inter_ymin < inter_ymax:
                    inter_area = (inter_xmax - inter_xmin) * (inter_ymax - inter_ymin)
                    cropped_person_area += inter_area
            if total_person_area == 0:
                return 0
            return 1 - cropped_person_area / total_person_area

        person_loss = compute_person_loss(person_list, crop_coords)

        return {
            "color_loss": color_loss,
            "texture_loss": texture_loss,
            "person_loss": person_loss
        }

    def visualize_crop_and_loss(self, image: np.ndarray,
                                crop_coords: Tuple[int, int, int, int],
                                person_list: List[Tuple[int, int, int, int]],face_list: List[Tuple[int, int, int, int]],
                                loss_dict: dict,
                                window_name: str = "Crop Loss Visualization",
                                save_path: str = None):
        vis_img = image.copy()
        xmin, ymin, xmax, ymax = crop_coords

        # âœ… è£å‰ªåŒºåŸŸæ¡†ï¼ˆçº¢è‰²ï¼ŒåŠ ç²—ï¼‰
        cv2.rectangle(vis_img, (xmin, ymin), (xmax, ymax), (0, 0, 255), thickness=15)

        # âœ… personæ¡†ï¼ˆç»¿è‰²ï¼ŒåŠ ç²—ï¼‰
        for (pxmin, pymin, pxmax, pymax) in person_list:
            cv2.rectangle(vis_img, (pxmin, pymin), (pxmax, pymax), (0, 255, 0), thickness=7)
        # è„¸
        for (fxmin, fymin, fxmax, fymax) in face_list:
            cv2.rectangle(vis_img, (fxmin, fymin), (fxmax, fymax), (255, 140, 0), thickness=7)

        # âœ… æ”¾å¤§å­—ä½“å¤§å°å’ŒåŠ ç²—
        info_text = f"Color: {loss_dict['color_loss']:.3f} | Texture: {loss_dict['texture_loss']:.3f} | Person: {loss_dict['person_loss']:.3f}"
        font_scale = 3
        font_thickness = 5
        cv2.putText(vis_img, info_text, (10, 70), cv2.FONT_HERSHEY_SIMPLEX,
                    font_scale, (255, 255, 255), font_thickness, cv2.LINE_AA)

        # âœ… ç¼©æ”¾æ˜¾ç¤ºï¼ˆé˜²æ­¢å¤ªå¤§ï¼‰
        max_width = 1280
        max_height = 1280
        h, w = vis_img.shape[:2]
        scale = min(max_width / w, max_height / h, 1.0)
        new_w, new_h = int(w * scale), int(h * scale)
        resized_img = cv2.resize(vis_img, (new_w, new_h))

        # âœ… æ˜¾ç¤ºå›¾åƒ
        cv2.imshow(window_name, resized_img)
        cv2.waitKey(0)
        cv2.destroyAllWindows()

    def process_image_list(self, save_dir):
        # è®¾ç½® CSV æ–‡ä»¶çš„è·¯å¾„
        csv_file_path = save_dir + '/output.csv' if save_dir else 'output.csv'

        # æ‰“å¼€ CSV æ–‡ä»¶è¿›è¡Œå†™å…¥
        with open(csv_file_path, mode='w', newline='') as csvfile:
            fieldnames = ["Image Name", "Padding", "Aspect Ratio", "xmin", "ymin", "xmax", "ymax"]
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

            # å†™å…¥è¡¨å¤´
            writer.writeheader()


            for idx, image_obj in enumerate(self.image_list):
                # æå–ç›¸å…³ä¿¡æ¯
                image_name = os.path.basename(image_obj.image_path)  # è·å–å›¾ç‰‡çš„æ–‡ä»¶å
                padding = image_obj.need_crop
                aspect_ratio = self.user_setting.ratio  # å‡è®¾ `self.user_setting.ratio` æ˜¯æ¯”ä¾‹è®¾ç½®
                if image_obj.crop_coordinates:
                    xmin, ymin, xmax, ymax = image_obj.crop_coordinates
                else:
                    xmin = ymin = xmax = ymax = ""
                # å°†æ¯è¡Œæ•°æ®å†™å…¥ CSV æ–‡ä»¶
                writer.writerow({
                    "Image Name": image_name,
                    "Padding": padding,
                    "Aspect Ratio": aspect_ratio,
                    "xmin": xmin,
                    "ymin": ymin,
                    "xmax": xmax,
                    "ymax": ymax
                })

                if not image_obj.need_crop == 1:
                    continue

                # print("å›¾ç‰‡idï¼š", image_obj.image_id)
                # image_np = image_obj.image
                # crop_coords = image_obj.crop_coordinates
                # person_boxes = image_obj.person_list
                # face_boxes = image_obj.face_list
                # loss = self.compute_crop_loss(image_np, crop_coords, person_boxes)
                # print("æŸå¤±ï¼š", loss)
                #
                #
                # self.visualize_crop_and_loss(
                #     image=image_np,
                #     crop_coords=crop_coords,
                #     person_list=person_boxes,
                #     face_list=face_boxes,
                #     loss_dict=loss,
                #     window_name=f"Image {idx + 1} Loss Visualization",
                # )

